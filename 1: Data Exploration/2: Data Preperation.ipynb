{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from openpyxl import load_workbook\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from scipy.stats import chisquare\n",
    "import math\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T18:06:32.687700910Z",
     "start_time": "2023-10-06T18:06:31.909969152Z"
    }
   },
   "id": "c877ead4d1e1764b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "xlsx_filename = 'Semtex_assignment_heat_demand_forecast.xlsx'\n",
    "tab_name = 'Input data'\n",
    "csv_path = 'Semtex_Assignment.csv'\n",
    "directory = 'Semtex_Assignment/0: Extract Data'\n",
    "openpyxl_path = 'Semtex_Assignment_openpyxl.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T18:06:32.704147617Z",
     "start_time": "2023-10-06T18:06:32.690614440Z"
    }
   },
   "id": "2f454e103cc526b8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T18:06:32.704593739Z",
     "start_time": "2023-10-06T18:06:32.697243050Z"
    }
   },
   "id": "35d008c764e58f7f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-06T18:06:33.062817087Z",
     "start_time": "2023-10-06T18:06:32.708902748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting CSV from XLSX...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/drfeelgood/PycharmProjects/Hylife_Assignment/0: Extract Data/Hylife_assignment_heat_demand_forecast.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124mExtracting CSV from XLSX...\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Load the Excel workbook\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m workbook \u001B[38;5;241m=\u001B[39m \u001B[43mload_workbook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxlsx_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Select the sheet to convert\u001B[39;00m\n\u001B[1;32m      9\u001B[0m sheet \u001B[38;5;241m=\u001B[39m workbook[tab_name]\n",
      "File \u001B[0;32m~/PycharmProjects/Hylife_Assignment/venv/lib/python3.10/site-packages/openpyxl/reader/excel.py:344\u001B[0m, in \u001B[0;36mload_workbook\u001B[0;34m(filename, read_only, keep_vba, data_only, keep_links, rich_text)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_workbook\u001B[39m(filename, read_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, keep_vba\u001B[38;5;241m=\u001B[39mKEEP_VBA,\n\u001B[1;32m    315\u001B[0m                   data_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, keep_links\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, rich_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    316\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Open the given filename and return the workbook\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \n\u001B[1;32m    318\u001B[0m \u001B[38;5;124;03m    :param filename: the path to open or a file-like object\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    342\u001B[0m \n\u001B[1;32m    343\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 344\u001B[0m     reader \u001B[38;5;241m=\u001B[39m \u001B[43mExcelReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_vba\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mdata_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_links\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrich_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m     reader\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m    347\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m reader\u001B[38;5;241m.\u001B[39mwb\n",
      "File \u001B[0;32m~/PycharmProjects/Hylife_Assignment/venv/lib/python3.10/site-packages/openpyxl/reader/excel.py:123\u001B[0m, in \u001B[0;36mExcelReader.__init__\u001B[0;34m(self, fn, read_only, keep_vba, data_only, keep_links, rich_text)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn, read_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, keep_vba\u001B[38;5;241m=\u001B[39mKEEP_VBA,\n\u001B[1;32m    122\u001B[0m              data_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, keep_links\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, rich_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 123\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marchive \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_archive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalid_files \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marchive\u001B[38;5;241m.\u001B[39mnamelist()\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_only \u001B[38;5;241m=\u001B[39m read_only\n",
      "File \u001B[0;32m~/PycharmProjects/Hylife_Assignment/venv/lib/python3.10/site-packages/openpyxl/reader/excel.py:95\u001B[0m, in \u001B[0;36m_validate_archive\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m     88\u001B[0m             msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopenpyxl does not support \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m file format, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     89\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplease check you can open \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     90\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mit with Excel first. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     91\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSupported formats are: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m%\u001B[39m (file_format,\n\u001B[1;32m     92\u001B[0m                                                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(SUPPORTED_FORMATS))\n\u001B[1;32m     93\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidFileException(msg)\n\u001B[0;32m---> 95\u001B[0m archive \u001B[38;5;241m=\u001B[39m \u001B[43mZipFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m archive\n",
      "File \u001B[0;32m/usr/lib/python3.10/zipfile.py:1251\u001B[0m, in \u001B[0;36mZipFile.__init__\u001B[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001B[0m\n\u001B[1;32m   1249\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m   1250\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1251\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp \u001B[38;5;241m=\u001B[39m \u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilemode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1252\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m   1253\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m filemode \u001B[38;5;129;01min\u001B[39;00m modeDict:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/drfeelgood/PycharmProjects/Hylife_Assignment/0: Extract Data/Hylife_assignment_heat_demand_forecast.xlsx'"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Extracting CSV from XLSX...\n",
    "\"\"\")\n",
    "\n",
    "# Load the Excel workbook\n",
    "workbook = load_workbook(filename=xlsx_filename, data_only=True)\n",
    "\n",
    "# Select the sheet to convert\n",
    "sheet = workbook[tab_name]\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "WARNING: Package openpyxl changes '-' values to '0'. A manual copy has been made to keep the faulty values in the data.\n",
    "WARNING: Manual save of CSV file does not compute millisecond range. Hard copy of range has been done to Hulife_Assignment.csv\n",
    "\"\"\")\n",
    "\n",
    "# Open a CSV file to write to\n",
    "with open(openpyxl_path, 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # Iterate over the rows in the sheet and write them to the CSV file\n",
    "    for row in sheet.iter_rows(min_row=1, max_col=sheet.max_column, max_row=sheet.max_row):\n",
    "        writer.writerow([cell.internal_value if cell.internal_value is not None else '' for cell in row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Making datetime uniform....\n",
    "\"\"\")\n",
    "\n",
    "def check_time_increments(file_path, time_column):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Initialize a variable to hold the last time\n",
    "    last_time = None\n",
    "\n",
    "    # Loop over each row in the specified column\n",
    "    for index, row in df.iterrows():\n",
    "        # Determine the correct format string based on whether the datetime string includes fractional seconds\n",
    "        date_str = row[time_column]\n",
    "        \n",
    "        # Identify the format of the date string\n",
    "        if '.' in date_str:  # Fractional seconds are present\n",
    "            if '-' in date_str:  # Format: YYYY-MM-DD HH:MM:SS.ffffff\n",
    "                date_format = '%Y-%m-%d %H:%M:%S.%f'\n",
    "            else:  # Format: MM/DD/YYYY HH:MM:SS.ffffff\n",
    "                date_format = '%m/%d/%Y %H:%M:%S.%f'\n",
    "        else:  # No fractional seconds\n",
    "            if '-' in date_str:  # Format: YYYY-MM-DD HH:MM\n",
    "                date_format = '%Y-%m-%d %H:%M'\n",
    "            else:  # Format: MM/DD/YYYY HH:MM\n",
    "                date_format = '%m/%d/%Y %H:%M'\n",
    "        \n",
    "        # Parse the string to a datetime object\n",
    "        current_time = datetime.strptime(date_str, date_format)\n",
    "        \n",
    "        # Reformat the date string to the desired format\n",
    "        desired_format = '%Y-%m-%d %H:%M:%S.%f'\n",
    "        formatted_date_str = current_time.strftime(desired_format)\n",
    "        \n",
    "        # Update the DataFrame with the reformatted date string\n",
    "        df.at[index, time_column] = formatted_date_str\n",
    "\n",
    "        if last_time is not None:\n",
    "            # Check if the current_time is less than last_time\n",
    "            if current_time < last_time:\n",
    "                print(f\"Out-of-order date at index {index}: {last_time.strftime(desired_format)} to {formatted_date_str}\")\n",
    "            # Check if the current_time is not one hour ahead of the last_time\n",
    "            elif current_time != last_time + timedelta(hours=1):\n",
    "                print(f\"Incorrect increment at index {index}: {last_time.strftime(desired_format)} to {formatted_date_str}\")\n",
    "\n",
    "        # Update the last_time to the current_time for the next iteration\n",
    "        last_time = current_time\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "# Replace with the path to your CSV file\n",
    "time_column = \"UTC\"\n",
    "check_time_increments(csv_path, time_column)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.061827338Z"
    }
   },
   "id": "1b2095a71304bea6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Calculating differences between faulty time delta.\n",
    "\"\"\")\n",
    "def calculate_differences(date_object):\n",
    "    # If the date is not at the beginning of the hour\n",
    "    if date_object.minute != 0 or date_object.second != 0 or date_object.microsecond != 0:\n",
    "        # Find the previous and next hour\n",
    "        previous_hour = date_object.replace(minute=0, second=0, microsecond=0)\n",
    "        next_hour = previous_hour + timedelta(hours=1)\n",
    "        \n",
    "        # Calculate the differences\n",
    "        difference_from_previous = date_object - previous_hour\n",
    "        difference_from_next = next_hour - date_object\n",
    "        return difference_from_previous, difference_from_next\n",
    "    return None, None\n",
    "\n",
    "\n",
    "date_column_index = 0  # Adjust if the dates are in a different column\n",
    "date_format = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "with open(csv_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    # Skip header\n",
    "    next(reader)\n",
    "    \n",
    "    for row_num, row in enumerate(reader, start=2):  # Starting from 2 to account for the header row\n",
    "        date_str = row[date_column_index]\n",
    "        try:\n",
    "            date_object = datetime.strptime(date_str, date_format)\n",
    "            diff_previous, diff_next = calculate_differences(date_object)\n",
    "            if diff_previous and diff_next:\n",
    "                print(f\"Row {row_num}: {date_object} is {diff_previous} away from the previous hour and {diff_next} away from the next hour.\")\n",
    "        except ValueError:\n",
    "            print(f\"Invalid date format at row {row_num}: {date_str}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T18:06:33.080727697Z",
     "start_time": "2023-10-06T18:06:33.066166517Z"
    }
   },
   "id": "fe201b8fb24711ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A change in the timestamp fractional seconds has been noted, starting from the `2022-01-01 01:59:59.990000` timestamp. The incremental difference is `0.005` seconds per hour, ultimately stabilizing at `0.150` seconds from the `2022-01-02 05:59:59.850000` timestamp onward.\n",
    "\n",
    "\n",
    "IF time was measured correctly, then there is a possibility for data to be set to seconds since epoch, allowing us to correct the faulty time intervals. This approach is not desired however, since we're talking about centiseconds on the hour and the project does not scale up (it's not a single space calculation done for many spaces).\n",
    "\n",
    "\n",
    "### Possible Causes:\n",
    "\n",
    "1. **Hardware Oscillator Inaccuracy:**\n",
    "   The internal oscillator of the IoT device might not be accurate, leading to a drift in the device's internal clock. Hardware oscillators have a tolerance level, usually measured in parts per million (ppm), which defines how much the oscillator can deviate from the accurate time.\n",
    "\n",
    "2. **Temperature Fluctuation:**\n",
    "   Since the device is a temperature IoT device, it’s susceptible to environmental conditions. Changes in temperature can impact the internal oscillator’s speed, causing the clock to drift.\n",
    "\n",
    "3. **Software Bug:**\n",
    "   There might be a bug in the software running on the IoT device that is causing this behavior. This could be related to incorrect handling of leap seconds or other time synchronization logic.\n",
    "\n",
    "4. **Time Synchronization Issues:**\n",
    "   If the device is synchronizing its time with an external time source and there is an issue or error in the synchronization process, it might lead to anomalies in the timestamp.\n",
    "\n",
    "5. **Manual Adjustment:**\n",
    "   The device’s time might be getting adjusted manually, either by a user or by some automated process, leading to such shifts in time.\n",
    "\n",
    "### Troubleshooting Steps:\n",
    "\n",
    "- **Time Synchronization:**\n",
    "  Ensure that the device’s clock is synchronized with a reliable time source, such as an NTP server, to correct for any drift.\n",
    "\n",
    "- **Environmental Conditions:**\n",
    "  Investigate the environmental conditions, like temperature and humidity, where the device is located to see if they are causing the drift.\n",
    "\n",
    "- **Software and Firmware Review:**\n",
    "  Examine the device's firmware or software for any potential bugs related to timekeeping or synchronization, and apply updates if available.\n",
    "\n",
    "- **Hardware Specifications Review:**\n",
    "  Review the specifications of the hardware, especially focusing on the oscillator's accuracy and tolerance, to understand if the hardware is the source of the drift.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Addressing the clock drift in IoT devices is crucial to maintain the accuracy and reliability of the timestamp data. Identifying the root cause, whether it's hardware inaccuracy, environmental conditions, software bugs, or synchronization issues, is the first step in resolving the time drift problem.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a2a10d2c138db3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Fixing faulty time to nearest hour...\n",
    "\"\"\")\n",
    "def round_to_nearest_hour(dt):\n",
    "    if dt.minute == 0 and dt.second == 0 and dt.microsecond == 0:\n",
    "        return dt\n",
    "    rounded_dt = dt.replace(minute=0, second=0, microsecond=0)\n",
    "    if dt.minute >= 30:\n",
    "        rounded_dt += timedelta(hours=1)\n",
    "    return rounded_dt\n",
    "\n",
    "def round_datetimes_in_csv(file_path, datetime_column_index):\n",
    "    with open(file_path, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Modify the datetime column\n",
    "    for i, row in enumerate(rows):\n",
    "        if i == 0:  # skip header row\n",
    "            continue\n",
    "        dt_str = row[datetime_column_index]\n",
    "        dt = datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        rounded_dt = round_to_nearest_hour(dt)\n",
    "        row[datetime_column_index] = rounded_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Write the modified rows back to the same file\n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# Usage example\n",
    "round_datetimes_in_csv(csv_path, 0)  # Change 0 to the index of the datetime column in your CSV\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.068749240Z"
    }
   },
   "id": "7829b236ef357038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_csv_file(file_path):\n",
    "    # Read the csv file into a DataFrame\n",
    "    df = pd.read_csv(file_path, parse_dates=['UTC'])\n",
    "    \n",
    "    # Iterate over the first 50 rows and update the 'UTC' column where needed\n",
    "    for index in range(min(50, len(df))):\n",
    "        if df.loc[index, 'UTC'].year == 2022:\n",
    "            # Update the year to 2021\n",
    "            updated_date = df.loc[index, 'UTC'].replace(year=2021)\n",
    "            df.at[index, 'UTC'] = updated_date\n",
    "    \n",
    "    # Write the updated DataFrame back to the csv file\n",
    "    df.to_csv(file_path, index=False, date_format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Replace csv_path with the path to your csv file\n",
    "\n",
    "update_csv_file(csv_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.071402311Z"
    }
   },
   "id": "9e68f6990e722ef6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Splitting CSV into its parts...\n",
    "\"\"\")\n",
    "\n",
    "def split_csv(file_path, save_directory):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Define the output file names\n",
    "    file1_name = 'Historic_Data.csv'\n",
    "    file2_name = 'Heat_Demand.csv'\n",
    "    file3_name = 'Forecast_Data.csv'\n",
    "\n",
    "    # Create full paths for the output files\n",
    "    file1_path = os.path.join(save_directory, file1_name)\n",
    "    file2_path = os.path.join(save_directory, file2_name)\n",
    "    file3_path = os.path.join(save_directory, file3_name)\n",
    "\n",
    "    # Find indices where all heat demand columns are not NULL\n",
    "    not_null_heat_demand_indices = data[~data[['S31 Heat demand (%)', 'S32 Heat demand (%)', 'S41 Heat demand (%)']].isnull().any(axis=1)].index\n",
    "\n",
    "    # Find indices where all heat demand columns are NULL but kWh is not NULL\n",
    "    null_heat_demand_not_null_kwh_indices = data[\n",
    "        data[['S31 Heat demand (%)', 'S32 Heat demand (%)', 'S41 Heat demand (%)']].isnull().any(axis=1) &\n",
    "        ~data['Building Semtex OFFICE (kWh)'].isnull()\n",
    "    ].index\n",
    "\n",
    "    # Find indices where both heat demand columns and kWh are NULL\n",
    "    null_heat_demand_and_kwh_indices = data[\n",
    "        data[['S31 Heat demand (%)', 'S32 Heat demand (%)', 'S41 Heat demand (%)', 'Building Semtex OFFICE (kWh)']].isnull().any(axis=1)\n",
    "    ].index\n",
    "\n",
    "    # Check if any non-NULL values appear after NULL values in the respective columns\n",
    "    if not null_heat_demand_not_null_kwh_indices.empty and max(null_heat_demand_not_null_kwh_indices) < max(null_heat_demand_and_kwh_indices):\n",
    "        print(\"Warning: non-NULL kWh values appear after NULL kWh values in the file.\")\n",
    "    if max(not_null_heat_demand_indices) > min(null_heat_demand_not_null_kwh_indices):\n",
    "        print(\"Warning: non-NULL heat demand values appear after NULL heat demand values in the file.\")\n",
    "\n",
    "    # Save the corresponding data frames to new CSV files\n",
    "    data.loc[not_null_heat_demand_indices].to_csv(file1_path, index=False)\n",
    "    data.loc[null_heat_demand_not_null_kwh_indices].to_csv(file2_path, index=False)\n",
    "    data.loc[null_heat_demand_and_kwh_indices].to_csv(file3_path, index=False)\n",
    "\n",
    "    print(f\"{file1_path} has been created with indices {min(not_null_heat_demand_indices)+1} to {max(not_null_heat_demand_indices)+1}\")\n",
    "    print(f\"{file2_path} has been created with indices {min(null_heat_demand_not_null_kwh_indices)+1} to {max(null_heat_demand_not_null_kwh_indices)+1}\")\n",
    "    print(f\"{file3_path} has been created with indices {min(null_heat_demand_and_kwh_indices)+1} to {max(null_heat_demand_and_kwh_indices)+1}\")\n",
    "\n",
    "split_csv(csv_path, directory)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.077531777Z"
    }
   },
   "id": "f9811641680a1fa3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.079712706Z"
    }
   },
   "id": "62555f5854d90327"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from os import path\n",
    "\n",
    "\"\"\"Analysis of Unusual Values\"\"\"\n",
    "\n",
    "# Define the path of the historic data file\n",
    "historic_data_file = path.join(directory, 'Historic_Data.csv')\n",
    "forecast_data_file = path.join(directory, 'Forecast_Data.csv')\n",
    "heat_demand_data_file = path.join(directory, 'Heat_Demand.csv')\n",
    "\n",
    "# Read the historic data file\n",
    "historic_data = pd.read_csv(historic_data_file)\n",
    "forecast_data = pd.read_csv(forecast_data_file)\n",
    "heat_demand_data = pd.read_csv(heat_demand_data_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.081655152Z"
    }
   },
   "id": "e1c2a7c0d8220aef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Checking for any faulty data, example \"object\" instead of \"float64\"....\n",
    "\"\"\")\n",
    "historic_data.info()\n",
    "forecast_data.info()\n",
    "heat_demand_data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.083417394Z"
    }
   },
   "id": "41b7fd7714caa57e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_unusual_values(df, excluded_columns=None):\n",
    "    \"\"\"\n",
    "    This function iterates over each cell in the input DataFrame to find unusual values\n",
    "    and returns a DataFrame containing these unusual values with their column names and indices.\n",
    "    \n",
    "    :param df: DataFrame, the input data frame to be checked for unusual values.\n",
    "    :param excluded_columns: list, optional, a list of columns to exclude from the check.\n",
    "    :return: DataFrame, a data frame containing the unusual values.\n",
    "    \"\"\"\n",
    "\n",
    "    if excluded_columns is None:\n",
    "        excluded_columns = [\"UTC\"]\n",
    "\n",
    "    unusual_values = {'column': [], 'index': [], 'value': []}\n",
    "\n",
    "    # Drop columns where all values are NaN\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in excluded_columns:  # Skip excluded columns\n",
    "            for idx, value in df[col].items():\n",
    "                if pd.isna(value):  # Skip NaN values\n",
    "                    continue\n",
    "                try:\n",
    "                    # Try to convert value to float\n",
    "                    float(value)\n",
    "                except ValueError:\n",
    "                    # If conversion fails, it's an unusual value\n",
    "                    unusual_values['column'].append(col)\n",
    "                    unusual_values['index'].append(idx)\n",
    "                    unusual_values['value'].append(value)\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    unusual_values_df = pd.DataFrame(unusual_values)\n",
    "    return unusual_values_df\n",
    "\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "unusual_values_historic_df = find_unusual_values(historic_data)\n",
    "unusual_values_forecast_df = find_unusual_values(forecast_data)\n",
    "unusual_values_heat_demand_df = find_unusual_values(heat_demand_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.085260215Z"
    }
   },
   "id": "c516110839f63339"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Function for extracting unique wrong values.\"\"\"\n",
    "\n",
    "\n",
    "def extract_unique_unusual_values(unusual_values_df, csv_file):\n",
    "    \"\"\"\n",
    "    This function extracts unique unusual values from the provided DataFrame\n",
    "    and returns a list of tuples containing the CSV file name, column name, and unique unusual values.\n",
    "    \n",
    "    :param unusual_values_df: DataFrame, the input data frame containing unusual values.\n",
    "    :param csv_file: str, the name of the original CSV file from which the DataFrame was created.\n",
    "    :return: list, a list of tuples containing the CSV file name, column name, and unique unusual values.\n",
    "    \"\"\"\n",
    "\n",
    "    unique_unusual_values_list = []\n",
    "    for column in unusual_values_df['column'].unique():\n",
    "        # Filter DataFrame for each unique column\n",
    "        column_df = unusual_values_df[unusual_values_df['column'] == column]\n",
    "\n",
    "        # Extract unique unusual values for the column\n",
    "        unique_unusual_values = column_df['value'].unique()\n",
    "        # Append results to the list\n",
    "        unique_unusual_values_list.append(csv_file)\n",
    "        unique_unusual_values_list.append(column)\n",
    "        unique_unusual_values_list.append(unique_unusual_values)\n",
    "\n",
    "    return unique_unusual_values_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.086943373Z"
    }
   },
   "id": "c41a44a067db6856"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Historic_Data.csv: Contains unusual values - in the Outside temp (0.1 °C) column.\"\"\"\n",
    "historic_faulty_values = extract_unique_unusual_values(unusual_values_historic_df, historic_data_file)\n",
    "historic_faulty_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.128821742Z"
    }
   },
   "id": "d28446287a8c1fcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Forecast_Data.csv: Contains NaN values in the columns S31 Heat demand (%), S32 Heat demand (%), S41 Heat demand (%), and Building Semtex OFFICE (kWh).\"\"\"\n",
    "forecast_faulty_values = extract_unique_unusual_values(unusual_values_forecast_df, forecast_data_file)\n",
    "forecast_faulty_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.128985315Z"
    }
   },
   "id": "fbc4f304b195f145"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Heat_Demand.csv: Contains NaN values in the columns S31 Heat demand (%), S32 Heat demand (%), and S41 Heat demand (%).\"\"\"\n",
    "heat_demand_faulty_values = extract_unique_unusual_values(unusual_values_heat_demand_df, heat_demand_data_file)\n",
    "heat_demand_faulty_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.129057037Z"
    }
   },
   "id": "99e958afcec2de9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_ = historic_faulty_values + forecast_faulty_values + heat_demand_faulty_values\n",
    "data_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T18:06:33.136764860Z",
     "start_time": "2023-10-06T18:06:33.129111246Z"
    }
   },
   "id": "c6e8d5d100ca57f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def analyze_unusual_values(data_list):\n",
    "    results = {}\n",
    "\n",
    "    for i in range(0, len(data_list), 3):\n",
    "        csv_file = data_list[i]\n",
    "        column_name = data_list[i + 1]\n",
    "        unusual_value = data_list[i + 2]\n",
    "\n",
    "        # Read the data file\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Extract rows with unusual values in the data\n",
    "        unusual_rows = data.loc[data[column_name].isin(unusual_value)].copy()\n",
    "\n",
    "        # Temporal Clustering Analysis\n",
    "        unusual_rows['UTC'] = pd.to_datetime(unusual_rows['UTC'])\n",
    "        time_diff = unusual_rows['UTC'].diff().dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "        # Frequency Analysis\n",
    "        total_entries = len(data)\n",
    "        erroneous_entries = len(unusual_rows)\n",
    "        valid_entries = total_entries - erroneous_entries\n",
    "\n",
    "        # Statistical Tests - Chi-square Test\n",
    "        observed_frequencies = [erroneous_entries, valid_entries]\n",
    "        expected_frequencies = [total_entries / 2, total_entries / 2]  # Assuming uniform distribution for the test\n",
    "        chi_square_stat, p_value = chisquare(observed_frequencies, expected_frequencies)\n",
    "\n",
    "        # Entropy Analysis\n",
    "        p_erroneous = erroneous_entries / total_entries\n",
    "        p_valid = valid_entries / total_entries\n",
    "\n",
    "        # Check for log(0) case\n",
    "        if erroneous_entries == 0 or valid_entries == 0:\n",
    "            entropy = 0\n",
    "        else:\n",
    "            entropy = -(p_erroneous * math.log2(p_erroneous) + p_valid * math.log2(p_valid))\n",
    "\n",
    "        # Date-Time Analysis\n",
    "        during_work_hours = unusual_rows[(unusual_rows['UTC'].dt.weekday < 5) & (unusual_rows['UTC'].dt.hour >= 9) & (\n",
    "                    unusual_rows['UTC'].dt.hour < 17)]\n",
    "        outside_work_hours = unusual_rows[(unusual_rows['UTC'].dt.weekday < 5) & (\n",
    "                    (unusual_rows['UTC'].dt.hour < 9) | (unusual_rows['UTC'].dt.hour >= 17))]\n",
    "        weekends = unusual_rows[unusual_rows['UTC'].dt.weekday >= 5]\n",
    "\n",
    "        # Adjust the categorization of the 9:00 and 17:00 timestamps\n",
    "        during_work_hours = pd.concat([during_work_hours, unusual_rows[\n",
    "            (unusual_rows['UTC'].dt.weekday < 5) & (unusual_rows['UTC'].dt.hour == 17)]])\n",
    "        outside_work_hours = pd.concat([outside_work_hours, unusual_rows[\n",
    "            (unusual_rows['UTC'].dt.weekday < 5) & (unusual_rows['UTC'].dt.hour == 9)]])\n",
    "\n",
    "        date_time_analysis = {\n",
    "            'During Work Hours': len(during_work_hours),\n",
    "            'Outside Work Hours': len(outside_work_hours),\n",
    "            'Weekends': len(weekends)\n",
    "        }\n",
    "\n",
    "        results[csv_file] = {\n",
    "            'Column Name': column_name,\n",
    "            'Unusual Value': unusual_value,\n",
    "            'Frequency Analysis': {\n",
    "                'Total Entries': total_entries,\n",
    "                'Erroneous Entries': erroneous_entries,\n",
    "                'Valid Entries': valid_entries\n",
    "            },\n",
    "            'Clustering Analysis': time_diff.describe().to_dict(),\n",
    "            'Statistical Tests': {\n",
    "                'Chi Square Statistic': chi_square_stat,\n",
    "                'P Value': p_value\n",
    "            },\n",
    "            'Entropy Analysis': {\n",
    "                'Entropy': entropy\n",
    "            },\n",
    "            'Date-Time Analysis': date_time_analysis\n",
    "        }\n",
    "\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.129171784Z"
    }
   },
   "id": "9a873da6b967d63f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysis_results = analyze_unusual_values(data_)\n",
    "\n",
    "# Display the results in a formatted manner\n",
    "for csv_file, result in analysis_results.items():\n",
    "    print(f\"CSV File: {csv_file}\")\n",
    "    print(f\"Column Name: {result['Column Name']}\")\n",
    "    print(f\"Unusual Value: {result['Unusual Value']}\")\n",
    "    print(\"Frequency Analysis:\", result['Frequency Analysis'])\n",
    "    print(\"Clustering Analysis:\", result['Clustering Analysis'])\n",
    "    print(\"Statistical Tests:\", result['Statistical Tests'])\n",
    "    print(\"Entropy Analysis:\", result['Entropy Analysis'])\n",
    "    print(\"Date-Time Analysis:\", result['Date-Time Analysis'])  # New line to print Date-Time Analysis results\n",
    "    print(\"---------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.129221803Z"
    }
   },
   "id": "bdcc84624d1ebc8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of Unusual Values in Historic Data\n",
    "\n",
    "- **Frequency Analysis:**\n",
    "   - Total Entries: 8760\n",
    "   - Erroneous Entries: 20\n",
    "   - Valid Entries: 8740\n",
    "   - Conclusion: A very small proportion of the dataset has erroneous entries.\n",
    "\n",
    "- **Temporal Clustering Analysis:**\n",
    "   - Mean Time Difference: 442.16 hours\n",
    "   - Standard Deviation: 1194.46 hours\n",
    "   - Minimum Time Difference: 1 hour\n",
    "   - Maximum Time Difference: 5327 hours\n",
    "   - Conclusion: Erroneous entries are spread out across the dataset without significant clustering.\n",
    "\n",
    "- **Statistical Tests:**\n",
    "   - Chi Square Statistic: 8680.18\n",
    "   - P Value: 0.0\n",
    "   - Conclusion: The occurrence of erroneous entries significantly differs from a uniform distribution, suggesting a non-random occurrence.\n",
    "\n",
    "- **Entropy Analysis:**\n",
    "   - Entropy: 0.0233\n",
    "   - Conclusion: The dataset is highly ordered, and the occurrence of erroneous entries is not completely random.\n",
    "\n",
    "- **Date-Time Analysis:**\n",
    "   - During Work Hours: 0\n",
    "   - Outside Work Hours: 12\n",
    "   - Weekends: 8\n",
    "   - Conclusion: Erroneous entries tend to occur outside work hours and on weekends, indicating a possible pattern related to operational hours.\n",
    "\n",
    "**Overall Conclusion:** The erroneous values are few and are not uniformly distributed. They tend to appear outside work hours and on weekends, indicating a higher degree of order in their occurrences.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ba5e9416740b0dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Date-Time Analysis on Erroneous Entries in Historic Data\n",
    "\n",
    "#### **CSV File:**\n",
    "`/0: Extract Data/Historic_Data.csv`\n",
    "\n",
    "#### **Column Name:**\n",
    "`Outside temp (0.1 °C)`\n",
    "\n",
    "#### **Unusual Value:**\n",
    "`[' -   ']`\n",
    "\n",
    "### **Analysis Results:**\n",
    "#### **Frequency Analysis:**\n",
    "- Total Entries: 8760\n",
    "- Erroneous Entries: 20\n",
    "- Valid Entries: 8740\n",
    "\n",
    "#### **Clustering Analysis:**\n",
    "- Count: 19.0\n",
    "- Mean: 442.16 hours\n",
    "- Std: 1194.46 hours\n",
    "- Min: 1.0 hour with a single continuous value of 2 hours.\n",
    "- 25%: 23.0 hours\n",
    "- 50%: 133.0 hours\n",
    "- 75%: 316.0 hours\n",
    "- Max: 5327.0 hours\n",
    "\n",
    "#### **Statistical Tests:**\n",
    "- Chi Square Statistic: 8680.18\n",
    "- P Value: 0.0\n",
    "\n",
    "#### **Entropy Analysis:**\n",
    "- Entropy: 0.0233\n",
    "\n",
    "#### **Date-Time Analysis:**\n",
    "- During Work Hours: 0\n",
    "- Outside Work Hours: 12\n",
    "- Weekends: 8\n",
    "\n",
    "### **Interpretation:**\n",
    "The Date-Time Analysis reveals that all the erroneous entries, represented by `-`, occur either outside typical work hours on weekdays or during weekends. Specifically:\n",
    "- No erroneous entries are recorded during the conventional work hours (9 AM to 5 PM on weekdays).\n",
    "- 12 erroneous entries are noted outside the standard work hours on weekdays.\n",
    "- 8 erroneous entries are registered on weekends.\n",
    "\n",
    "### **Conclusion:**\n",
    "The exclusive presence of erroneous entries during non-operational hours and their absence during operational hours suggest several possibilities:\n",
    "1. **Maintenance or Updates:** The system could be undergoing scheduled maintenance or updates during non-work hours, leading to disruptions in recording data.\n",
    "2. **Connection Loss or Disruptions:** The occurrences of erroneous entries during non-operational hours might be indicative of a loss of connection or other disruptions, potentially due to reduced monitoring or operational levels during these times.\n",
    "3. **Power Loss or Depleted Battery:** If the sensing device relies on a power source that is turned off during non-operational hours and weekends, it could be indicative of a depleted battery or power loss scenario.\n",
    "\n",
    "Further investigation involving system logs, maintenance records, or other pertinent information sources would be essential to pinpoint the specific cause of these erroneous entries.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd5ea1ceb03c04fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysis_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.129273830Z"
    }
   },
   "id": "468c95a3442d2ffe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_and_save_data(file_path, output_file_path):\n",
    "    # Load the dataset\n",
    "    historic_data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preprocessing: Replace '-' with NaN and convert column to numeric\n",
    "    historic_data.loc[historic_data['Outside temp (0.1 °C)'] == '-', 'Outside temp (0.1 °C)'] = float('nan')\n",
    "    historic_data['Outside temp (0.1 °C)'] = pd.to_numeric(historic_data['Outside temp (0.1 °C)'], errors='coerce')\n",
    "    \n",
    "    # Convert 'UTC' column to datetime\n",
    "    historic_data['UTC'] = pd.to_datetime(historic_data['UTC'])\n",
    "    \n",
    "    # Save cleaned data to a new CSV file\n",
    "    historic_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Usage\n",
    "input_file_path = 'Historic_Data.csv'\n",
    "output_file_path = 'Cleaned_Historic_Data.csv'\n",
    "\n",
    "clean_and_save_data(input_file_path, output_file_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.129320542Z"
    }
   },
   "id": "d04897490322d4cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T18:06:33.129364671Z"
    }
   },
   "id": "2f5b2eb31b3507d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
